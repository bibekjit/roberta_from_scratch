## ROBERTA Implementation

model config
`L=2` `H=256` `A=8` `vocab_size=20020` `maxlen=128` `ff_units=1024` 

15% dynamic masking (whole sequence is masked if sequence is very small)

tokenizer config
